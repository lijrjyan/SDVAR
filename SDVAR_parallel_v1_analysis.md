# SDVARå¹¶è¡ŒéªŒè¯ v1.0 æ·±åº¦åˆ†ææŠ¥å‘Š

## ğŸ“‹ é¡¹ç›®èƒŒæ™¯ç†è§£

### VARæ¨¡å‹æ ¸å¿ƒæœºåˆ¶
- **Next-Scale Prediction**: VARä½¿ç”¨é€å°ºåº¦ç”Ÿæˆè€Œéé€tokenç”Ÿæˆ
- **10ä¸ªå°ºåº¦**: patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
- **Tokenç´¯ç§¯**: 1Â² + 2Â² + 3Â² + ... + 16Â² = 680 tokens total
- **VQ-VAEé‡åŒ–**: Cvae=32, V=4096, ä½¿ç”¨æ®‹å·®é‡åŒ–

### SDVARæ¨æµ‹è§£ç æ€æƒ³
- **Draftæ¨¡å‹**: VAR-D16 (å¿«é€Ÿä½†è´¨é‡ç¨ä½)
- **Targetæ¨¡å‹**: VAR-D30 (ç²¾ç¡®ä½†é€Ÿåº¦æ…¢)
- **ç›®æ ‡**: å‡å°‘Targetæ¨¡å‹è°ƒç”¨æ¬¡æ•°ï¼Œä»10æ¬¡é™åˆ°2-5æ¬¡

---

## ğŸ” å½“å‰å®ç°åˆ†æ

### âœ… å·²æ­£ç¡®å®ç°çš„éƒ¨åˆ†

1. **åŸºç¡€æ¶æ„è®¾è®¡**
   - `SDVARInferenceState`ç±»æ­£ç¡®ç®¡ç†çŠ¶æ€
   - whileå¾ªç¯æ›¿ä»£forå¾ªç¯ï¼Œæ”¯æŒÎ³æ‰¹å¤„ç†
   - åŸºæœ¬çš„é”™è¯¯å¤„ç†å’Œå›æ»šæœºåˆ¶

2. **æ¨ç†æµç¨‹æ¡†æ¶**
   - `_initialize_inference_state`: çŠ¶æ€åˆå§‹åŒ–æ­£ç¡®
   - `draft_generate_batch`: æ‰¹é‡ç”ŸæˆåŸºæœ¬é€»è¾‘æ­£ç¡®
   - `target_verify_batch`: æ‰¹é‡éªŒè¯æ¡†æ¶æ­£ç¡®
   - `basic_token_matching`: ç®€å•åŒ¹é…é€»è¾‘å¯ç”¨

---

## ğŸš¨ å…³é”®é—®é¢˜è¯†åˆ«

### 1. **`_build_combined_query`å‡½æ•° - ä¸¥é‡é—®é¢˜**

```python
def _build_combined_query(self, draft_tokens: List[torch.Tensor], state, B: int) -> torch.Tensor:
```

**é—®é¢˜åˆ†æ**:
- âŒ **ä½ç½®ç¼–ç æ··ä¹±**: `current_pos`è®¡ç®—ä¸å‡†ç¡®ï¼Œæ²¡æœ‰æ­£ç¡®å¤„ç†ç´¯ç§¯tokenä½ç½®
- âŒ **CFGå¤„ç†é”™è¯¯**: åº”è¯¥åœ¨æ„å»ºæ—¶å°±åšCFG doublingï¼Œè€Œä¸æ˜¯æœ€å
- âŒ **ç¼ºå°‘é¦–å±‚å¤„ç†**: å½“`current_stage=0`æ—¶ï¼Œfirst_token_mapå¤„ç†ä¸å®Œæ•´
- âŒ **VAE embeddingè·¯å¾„é”™è¯¯**: ç›´æ¥ä½¿ç”¨draft tokensçš„embeddingå¯èƒ½ä¸ä¸€è‡´

**å¿…é¡»ä¿®å¤çš„å…·ä½“é—®é¢˜**:
```python
# å½“å‰ä»£ç é—®é¢˜
current_pos = sum(p**2 for p in state.patch_nums[:state.current_stage])
# åº”è¯¥æ”¹ä¸ºï¼š
cumulative_pos = sum(p**2 for p in state.patch_nums[:state.current_stage])
```

### 2. **`_split_logits_by_stage`å‡½æ•° - ä¸­ç­‰é—®é¢˜**

```python
def _split_logits_by_stage(self, target_logits: torch.Tensor, draft_tokens: List[torch.Tensor], B: int, state) -> List[torch.Tensor]:
```

**é—®é¢˜åˆ†æ**:
- âŒ **CFGç»´åº¦å¤„ç†**: æ²¡æœ‰æ­£ç¡®å¤„ç†2Bç»´åº¦çš„logitsåˆ†å‰²
- âŒ **è¾¹ç•Œæ£€æŸ¥ç¼ºå¤±**: æ²¡æœ‰æ£€æŸ¥logitsé•¿åº¦æ˜¯å¦åŒ¹é…é¢„æœŸ
- âš ï¸ **ç´¢å¼•è®¡ç®—**: prefix_lengthè®¡ç®—å¯èƒ½åœ¨æŸäº›æƒ…å†µä¸‹å‡ºé”™

### 3. **`_get_attention_mask`å‡½æ•° - å¾…å®Œå–„**

```python
def _get_attention_mask(self, mask_length: int, current_stage: int, gamma: int) -> torch.Tensor:
```

**é—®é¢˜åˆ†æ**:
- âŒ **Week 1è®¾è®¡ä¸å®Œæ•´**: åªæ˜¯ç®€å•çš„å› æœæ©ç ï¼Œæ²¡æœ‰è€ƒè™‘VARçš„ç‰¹æ®Šç»“æ„
- âŒ **æ²¡æœ‰è€ƒè™‘Î³æ‰¹å¤„ç†**: æ‰¹é‡éªŒè¯æ—¶çš„æ©ç ç­–ç•¥ä¸æ˜ç¡®
- âŒ **ç¼ºå°‘å—çº§æ©ç **: æ²¡æœ‰å®ç°ç±»ä¼¼test3å‡½æ•°ä¸­çš„sd_maskç­–ç•¥

### 4. **`draft_generate_batch`å‡½æ•° - è½»å¾®é—®é¢˜**

**é—®é¢˜åˆ†æ**:
- âš ï¸ **çŠ¶æ€æ›´æ–°ä¸ä¸€è‡´**: f_hatæ›´æ–°å¯èƒ½ä¸åŒæ­¥
- âš ï¸ **KV Cacheç®¡ç†**: æ²¡æœ‰æ˜ç¡®çš„cacheæ¸…ç†ç­–ç•¥
- âš ï¸ **è¾¹ç•Œæ¡ä»¶**: æœ€åä¸€å±‚å¤„ç†å¯èƒ½æœ‰é—®é¢˜

### 5. **`update_state_with_accepted_tokens`å‡½æ•° - ä¸¥é‡é—®é¢˜**

```python
def update_state_with_accepted_tokens(self, draft_tokens: List[torch.Tensor], accept_length: int, state, B: int):
```

**é—®é¢˜åˆ†æ**:
- âŒ **åŒé‡æ›´æ–°**: åœ¨main loopå’Œè¿™ä¸ªå‡½æ•°ä¸­éƒ½æ›´æ–°f_hatï¼Œå¯èƒ½å¯¼è‡´ä¸ä¸€è‡´
- âŒ **çŠ¶æ€åŒæ­¥**: draft_f_hatå’Œtarget_f_hatå¯èƒ½ä¸åŒæ­¥
- âŒ **token bufferç®¡ç†**: æ²¡æœ‰æ­£ç¡®ç»´æŠ¤accepted_tokensåˆ—è¡¨

---

## ğŸ› ï¸ ç´§æ€¥ä¿®å¤æ¸…å•

### ä¼˜å…ˆçº§ 1: å¿…é¡»ç«‹å³ä¿®å¤
1. **ä¿®å¤`_build_combined_query`**: æ­£ç¡®çš„ä½ç½®ç¼–ç å’ŒCFGå¤„ç†
2. **ä¿®å¤`_split_logits_by_stage`**: æ­£ç¡®çš„CFGç»´åº¦åˆ†å‰²
3. **ä¿®å¤çŠ¶æ€æ›´æ–°é€»è¾‘**: é¿å…f_hatçš„åŒé‡æ›´æ–°

### ä¼˜å…ˆçº§ 2: é‡è¦æ”¹è¿›
1. **å®Œå–„attention mask**: å®ç°åŸºç¡€çš„å—çº§æ©ç 
2. **æ”¹è¿›é”™è¯¯å¤„ç†**: æ·»åŠ æ›´å¤šè¾¹ç•Œæ£€æŸ¥
3. **ä¼˜åŒ–KV Cache**: æ˜ç¡®çš„cacheç®¡ç†ç­–ç•¥

### ä¼˜å…ˆçº§ 3: åŠŸèƒ½å¢å¼º
1. **æ·»åŠ è°ƒè¯•ä¿¡æ¯**: æ›´è¯¦ç»†çš„å½¢çŠ¶å’Œå€¼æ£€æŸ¥
2. **æ€§èƒ½ä¼˜åŒ–**: å‡å°‘ä¸å¿…è¦çš„tensoræ‹·è´
3. **æ‰©å±•æµ‹è¯•**: æ›´å…¨é¢çš„å•å…ƒæµ‹è¯•

---

## ğŸ“‹ å…·ä½“ä¿®å¤æ–¹æ¡ˆ

### 1. ä¿®å¤`_build_combined_query`å‡½æ•°

```python
def _build_combined_query(self, draft_tokens: List[torch.Tensor], state, B: int) -> torch.Tensor:
    """ä¿®å¤ç‰ˆæœ¬çš„combined queryæ„å»º"""
    
    # 1. æ­£ç¡®è®¡ç®—ä½ç½®åç§»
    base_pos = sum(p**2 for p in state.patch_nums[:state.current_stage])
    
    # 2. æ„å»ºå®Œæ•´è¾“å…¥åºåˆ—
    all_embeddings = []
    
    # 3. å¤„ç†ä¹‹å‰çš„tokens (å¦‚æœæœ‰)
    if state.current_stage > 0:
        # ä»stateä¸­è·å–ä¹‹å‰æ¥å—çš„tokens
        # TODO: éœ€è¦åœ¨stateä¸­æ­£ç¡®ç»´æŠ¤è¿™äº›ä¿¡æ¯
        pass
    
    # 4. å¤„ç†draft tokens
    current_pos = base_pos
    for stage_idx, tokens in enumerate(draft_tokens):
        current_stage = state.current_stage + stage_idx
        pn = state.patch_nums[current_stage]
        
        # æ­£ç¡®çš„embeddingè·¯å¾„
        h_BChw = self.target_model.vae_quant_proxy[0].embedding(tokens)
        h_embed = h_BChw.transpose(1, 2)  # B, pn*pn, Cvae
        
        # word embedding
        stage_embedding = self.target_model.word_embed(h_embed)
        
        # æ­£ç¡®çš„ä½ç½®ç¼–ç 
        pos_embed = state.target_lvl_pos[:, current_pos:current_pos + pn*pn]
        stage_embedding = stage_embedding + pos_embed
        
        all_embeddings.append(stage_embedding)
        current_pos += pn * pn
    
    # 5. æ‹¼æ¥å’ŒCFGå¤„ç†
    combined = torch.cat(all_embeddings, dim=1)
    combined = combined.repeat(2, 1, 1)  # CFG doubling
    
    return combined
```

### 2. ä¿®å¤`_split_logits_by_stage`å‡½æ•°

```python
def _split_logits_by_stage(self, target_logits: torch.Tensor, draft_tokens: List[torch.Tensor], B: int, state) -> List[torch.Tensor]:
    """ä¿®å¤ç‰ˆæœ¬çš„logitsåˆ†å‰²"""
    
    # 1. æ­£ç¡®å¤„ç†CFGç»´åº¦
    assert target_logits.shape[0] == 2 * B, f"Expected 2*B={2*B}, got {target_logits.shape[0]}"
    
    # 2. è®¡ç®—æ­£ç¡®çš„èµ·å§‹ä½ç½®
    base_pos = sum(p**2 for p in state.patch_nums[:state.current_stage])
    
    # 3. åˆ†å‰²æ¯ä¸ªstage
    logits_per_stage = []
    current_pos = base_pos
    
    for stage_idx, tokens in enumerate(draft_tokens):
        current_stage = state.current_stage + stage_idx
        pn = state.patch_nums[current_stage]
        stage_length = pn * pn
        
        # æå–logits
        stage_logits = target_logits[:, current_pos:current_pos + stage_length, :]
        logits_per_stage.append(stage_logits)
        current_pos += stage_length
    
    return logits_per_stage
```

---

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•æ¸…å•
- [ ] æµ‹è¯•`_build_combined_query`çš„è¾“å‡ºå½¢çŠ¶
- [ ] æµ‹è¯•`_split_logits_by_stage`çš„ä¸€è‡´æ€§
- [ ] æµ‹è¯•çŠ¶æ€æ›´æ–°çš„æ­£ç¡®æ€§
- [ ] æµ‹è¯•è¾¹ç•Œæƒ…å†µå¤„ç†

### 2. é›†æˆæµ‹è¯•æ¸…å•
- [ ] ä¸test3å‡½æ•°ç»“æœå¯¹æ¯”
- [ ] æ€§èƒ½æŒ‡æ ‡æµ‹è¯•
- [ ] å†…å­˜ä½¿ç”¨æµ‹è¯•
- [ ] é”™è¯¯å¤„ç†æµ‹è¯•

### 3. è°ƒè¯•å·¥å…·
- [ ] æ·»åŠ è¯¦ç»†çš„shapeæ‰“å°
- [ ] æ·»åŠ ä¸­é—´ç»“æœä¿å­˜
- [ ] æ·»åŠ æ€§èƒ½è®¡æ—¶
- [ ] æ·»åŠ å†…å­˜ç›‘æ§

---

## ğŸ¯ é¢„æœŸç»“æœ

### ä¿®å¤åçš„æ€§èƒ½ç›®æ ‡
- **Targetè°ƒç”¨æ¬¡æ•°**: ä»10æ¬¡é™åˆ°2-5æ¬¡ âœ“
- **å›¾åƒè´¨é‡**: FIDæŸå¤± < 0.1 âœ“
- **æ¨ç†é€Ÿåº¦**: 1.3x-1.7xåŠ é€Ÿ âœ“
- **å†…å­˜ä½¿ç”¨**: å¢åŠ  < 20% âœ“

### è´¨é‡ä¿è¯
- **ä¸€è‡´æ€§**: ä¸test3å‡½æ•°ç»“æœé«˜åº¦ä¸€è‡´
- **ç¨³å®šæ€§**: æ— å†…å­˜æ³„éœ²ï¼Œæ— å´©æºƒ
- **å¯æ‰©å±•æ€§**: ä¸ºWeek 2çš„é«˜çº§åŠŸèƒ½åšå‡†å¤‡

---

## ğŸ“ ç«‹å³è¡ŒåŠ¨å»ºè®®

1. **ç«‹å³ä¿®å¤**: ä»`_build_combined_query`å¼€å§‹
2. **é€æ­¥éªŒè¯**: æ¯ä¿®å¤ä¸€ä¸ªå‡½æ•°å°±æµ‹è¯•ä¸€æ¬¡
3. **ä¿æŒç®€å•**: Week 1ç›®æ ‡æ˜¯è®©åŸºç¡€åŠŸèƒ½æ­£å¸¸è¿è¡Œ
4. **è®°å½•é—®é¢˜**: ä¸ºWeek 2çš„æ”¹è¿›åšå‡†å¤‡

è¿™ä¸ªåˆ†ææŠ¥å‘Šæä¾›äº†å®Œæ•´çš„ä¿®å¤è·¯å¾„ï¼Œå»ºè®®ä»ä¼˜å…ˆçº§1çš„é—®é¢˜å¼€å§‹é€ä¸€è§£å†³ã€‚ 